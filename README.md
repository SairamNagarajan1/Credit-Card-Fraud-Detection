
> âœ… High accuracy and precision, but recall suggests some frauds still get missed â€” classic case of class imbalance!

---

## ğŸ“ˆ What I Learned

- The importance of **precision & recall over raw accuracy** in fraud detection
- How to use **confusion matrices** and **Matthews Correlation Coefficient (MCC)**
- Why **imbalanced data** requires special attention in real-world ML problems

---

## ğŸš€ Future Plans

- Implement **SMOTE** or **undersampling** to handle imbalance better
- Compare **Logistic Regression, XGBoost, and LightGBM**
- Try **autoencoder-based anomaly detection**

---

## ğŸ› ï¸ Tech Stack

- Python
- NumPy, Pandas
- Seaborn, Matplotlib
- Scikit-learn (Random Forest, metrics, train_test_split)

---

## ğŸ’¡ How to Run This Project

1. Clone this repo
2. Download the dataset from [Kaggle](https://www.kaggle.com/datasets/mlg-ulb/creditcardfraud)
3. Place `creditcard.csv` in the project directory
4. Run:

```bash
python fraud_detection.py

